{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introducción a las convnets: Clasificando números\n",
    "\n",
    "Esta libreta contiene un ejemplo sencillo que podeis encontrar con mas detalle en el capítulo 5 del libro [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n",
    "\n",
    "----\n",
    "\n",
    "Vamos a echarle un vistazo a un ejemplo sencillo de una convnet. La utilizaremos para clasificar el dataset MNIST, que es un dataset abierto que contiene números escritos a mano. \n",
    "\n",
    "![Números escritos a mano del dataset MNIST](http://corochann.com/wp-content/uploads/2017/02/mnist_plot.png)\n",
    "\n",
    "Las 6 líneas de código que puedes ver aqui debajo muestran que aspecto tiene una convnet basica. Es una pila de capas `Conv2D` y `MaxPooling2D`. Veremos en un rato más concretamente lo que hacen. \n",
    "Lo importante es notar que una convnet toma como input tensores de tamaño `(altura_imagen, anchura_imagen, canales_imagen)`. \n",
    "En nuestro caso, vamos a configurar nuestra convnet para procesar inputs de tamaño `(28, 28, 1)`,  que es el formato de las imágenes MNIST. \n",
    "Por eso le pasamos el argumento input_shape=(28, 28, 1)` a nuestra primera capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que pinta tiene nuestra arquitectura hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puedes ver arriba que la salida de cada capa `Conv2D` y `MaxPooling2D` es un tensor 3D de dimensiones `(altura, anchura, canales)`. La anchura y la altura tienden a diminuir según vamos yendo mas profundo en la red. El número de canales está controlado por el primer argumento que se le pasa a \n",
    "las capas `Conv2D`  (e.j. 32 o 64).\n",
    "\n",
    "El siguiente paso sería darle nuestro ultimo tensor (de dimensiones `(3, 3, 64)`) como entrada a una red densamente conectada. \n",
    "Estos clasificadores procesan vectores, que son 1D,  mientras que nuestra salida es un tensor 3D. \n",
    "Así que primero tendremos que aplanar nuestra salida 3D y convertirla en 1D y después añadir unas cuantas capas densas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos clasificar 10 categorías, lo que significa que nuestra capa final debe tener 10 nodos y una función de activación softmax. Vamos a ver que pinta tiene nuestra red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, nuestra salida de dimension `(3, 3, 64)` han sido aplanadas hasta convertirse en vectores de dimensión `(576,)`, antes de entrar en las dos capas densas.\n",
    "\n",
    "Vamos ahora a entrenar nuestra red con las imágenes del dataset MNIST. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del dataset de training antes de preprocesarlo:  (60000, 28, 28) \n",
      "\n",
      "Estas son las etiquetas de training:  [5 0 4 ..., 5 6 8] \n",
      "\n",
      "Estas son las imágenes (matrices) de training: \n",
      " \n",
      " [[[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " ..., \n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBorO4ntri5iiZobcKZXHRNxwM/U1BWjYaLc6jpupX8LRiHT40km3E5O5goAwDzznnHQ1nV6J4a0i4vPg/4lewsrq8vLnUbWEx28ZcqiAvuwOepI/KsSD4b+MJ4RM2hXNvCefMuytuo4z1kKiul13TY/CHwbtbA3drc3mu6k07y2jrInlQDbs3j72GYHjjJP4+Y1astTv8ATWdrC+ubVnADGCVkLfXB5pt3f3moS+be3c9zJ03zSFz+Zru/iMJV8K+Al3Yt/wCxgUQdA5b5j9T8v5V55RRVie/vLqC3guLueaG2UpBHJIWWJSckKD90Z9Kr1//Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Shape del dataset de training antes de preprocesarlo: \", train_images.shape, \"\\n\")\n",
    "\n",
    "print (\"Estas son las etiquetas de training: \", train_labels, \"\\n\")\n",
    "\n",
    "print (\"Estas son las imágenes (matrices) de training: \\n \\n\", train_images, \"\\n\")\n",
    "\n",
    "index=2\n",
    "prueba=train_images[index][:][:]\n",
    "\n",
    "import scipy.misc\n",
    "scipy.misc.imsave('numbers/input.jpg', prueba)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"numbers/input.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 5 6 8]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "Shape del dataset de training después del preprocesado:  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "print(train_labels)\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "print(train_labels)\n",
    "print(\"Shape del dataset de training después del preprocesado: \", train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 30000 samples\n",
      "Epoch 1/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.2658 - acc: 0.9175 - val_loss: 0.0844 - val_acc: 0.9740\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0715 - acc: 0.9784 - val_loss: 0.0658 - val_acc: 0.9802\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0466 - acc: 0.9855 - val_loss: 0.0591 - val_acc: 0.9823\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0338 - acc: 0.9897 - val_loss: 0.0525 - val_acc: 0.9847\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0253 - acc: 0.9917 - val_loss: 0.0479 - val_acc: 0.9861\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0507 - val_acc: 0.9865\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0167 - acc: 0.9944 - val_loss: 0.0487 - val_acc: 0.9878\n",
      "Epoch 8/10\n",
      "30000/30000 [==============================] - 28s - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0536 - val_acc: 0.9864\n",
      "Epoch 9/10\n",
      "30000/30000 [==============================] - 28s - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0552 - val_acc: 0.9867\n",
      "Epoch 10/10\n",
      "30000/30000 [==============================] - 29s - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0755 - val_acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history=model.fit(train_images, train_labels, validation_split=0.1, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a evaluar el modelo con las imágenes de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9856/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de nuestro test con una convnet básica es:  98.64 % ¡No está mal!\n"
     ]
    }
   ],
   "source": [
    "print(\"La precisión de nuestro test con una convnet básica es: \" , test_acc*100, \"% ¡No está mal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'seven.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b8714f2b7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seven.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size)\u001b[0m\n\u001b[1;32m    320\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    321\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seven.png'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_width=28\n",
    "img_height=28\n",
    "\n",
    "img = image.load_img('seven.png', target_size=(img_width, img_height),grayscale=True)\n",
    "x= image.img_to_array(img)\n",
    "print(\"image shape\", x.shape)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print(\"x shape\", x.shape)\n",
    "\n",
    "print(\"La imagen representa el número: \", model.predict_classes(x)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('net_numbers.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "La imagen representa el número:  6\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "newmodel = load_model('net_numbers.h5')\n",
    "print(\"La imagen representa el número: \", newmodel.predict_classes(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4af38b12b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEENfkgpi6EtSQQx9SSrI/wccg+b/AgKDUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b82e77a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "#plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
